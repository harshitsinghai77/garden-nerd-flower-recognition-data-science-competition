{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  category\n",
       "0         0        77\n",
       "1         1        81\n",
       "2         2        52\n",
       "3         3        72\n",
       "4         4        58"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = list(dataset['image_id'].values)\n",
    "img_labels = list(dataset['category'].values)\n",
    "dataset = dataset.applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '100', '101', '102', '11', '12', '13', '14', '15', '16',\n",
       "       '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26',\n",
       "       '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36',\n",
       "       '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46',\n",
       "       '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56',\n",
       "       '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66',\n",
       "       '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76',\n",
       "       '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86',\n",
       "       '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96',\n",
       "       '97', '98', '99'], dtype='<U3')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_to_be_created = np.unique(list(dataset['category']))\n",
    "folders_to_be_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_path in folders_to_be_created:\n",
    "    if not os.path.exists(\".//\" + new_path):\n",
    "        os.makedirs(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = folders_to_be_created.copy()\n",
    "path_to_current_file = \"C:/Users/shadow/Documents/Projects/HE_Challenge_data/data/train/\"\n",
    "path_to_new_destination_folder = \"C:/Users/shadow/Documents/Projects/HE_Challenge_data/data/train_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(file_names)):\n",
    "    current_img = str(file_names[f])+'.jpg'\n",
    "    current_label = str(img_labels[f])\n",
    "    shutil.move(path_to_current_file+current_img, path_to_new_destination_folder+current_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    5,    19,    32,   345,   350,   363,   389,   400,   702,\n",
       "         705,   734,   737,  1067,  1078,  1090,  1095,  1110,  1434,\n",
       "        1448,  1462,  1486,  1811,  1861,  2224,  2242,  2649,  4619,\n",
       "        4632,  4988,  4990,  5002,  5025,  5042,  5368,  5375,  5404,\n",
       "        5410,  5760,  5776,  5788,  5790,  5805,  6149,  6164,  6177,\n",
       "        6227,  6294,  6502,  7124,  7167,  7521,  7522,  7753,  7814,\n",
       "        7911,  7948,  8016,  8363,  8432,  8648,  9059,  9091,  9163,\n",
       "        9203,  9284,  9351,  9397,  9490,  9608,  9993, 10150, 10322,\n",
       "       10434, 10724, 10746, 10991, 11269, 11581, 11594, 11877, 12039,\n",
       "       12050, 12137, 12162, 12230, 12242, 12382, 12464, 12471, 12544,\n",
       "       12615, 12673, 12720, 12779, 13393, 13401, 13620, 14186, 14314,\n",
       "       14560, 14561, 14582, 14747, 14760, 14793, 14912, 15034, 15062,\n",
       "       15090, 15163, 15237, 15350, 15362, 15507, 15713, 15732, 15902,\n",
       "       16204, 16225, 16281, 16345, 16369, 16672, 16823, 16920, 17000,\n",
       "       17172, 17496, 17870, 17872, 17932, 18041, 18068, 18175, 18194],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['category'] == 2]['image_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 17 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-839479ebdd78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vgg16.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    144\u001b[0m       h5py is not None and (\n\u001b[0;32m    145\u001b[0m           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;31m# set weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    735\u001b[0m                      \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                      \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m                      ' layers.')\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 17 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, testing_data_pct):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    num_training_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "\n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            if np.random.rand(1) < testing_data_pct:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refusing to delete testing data directory data/validation as we prevent you from doing stupid things!\n",
      "Refusing to delete testing data directory data/train as we prevent you from doing stupid things!\n",
      "train_data vs train_data\n",
      "1 vs train_data\n",
      "10 vs train_data\n",
      "100 vs train_data\n",
      "101 vs train_data\n",
      "102 vs train_data\n",
      "11 vs train_data\n",
      "12 vs train_data\n",
      "13 vs train_data\n",
      "14 vs train_data\n",
      "15 vs train_data\n",
      "16 vs train_data\n",
      "17 vs train_data\n",
      "18 vs train_data\n",
      "19 vs train_data\n",
      "2 vs train_data\n",
      "20 vs train_data\n",
      "21 vs train_data\n",
      "22 vs train_data\n",
      "23 vs train_data\n",
      "24 vs train_data\n",
      "25 vs train_data\n",
      "26 vs train_data\n",
      "27 vs train_data\n",
      "28 vs train_data\n",
      "29 vs train_data\n",
      "3 vs train_data\n",
      "30 vs train_data\n",
      "31 vs train_data\n",
      "32 vs train_data\n",
      "33 vs train_data\n",
      "34 vs train_data\n",
      "35 vs train_data\n",
      "36 vs train_data\n",
      "37 vs train_data\n",
      "38 vs train_data\n",
      "39 vs train_data\n",
      "4 vs train_data\n",
      "40 vs train_data\n",
      "41 vs train_data\n",
      "42 vs train_data\n",
      "43 vs train_data\n",
      "44 vs train_data\n",
      "45 vs train_data\n",
      "46 vs train_data\n",
      "47 vs train_data\n",
      "48 vs train_data\n",
      "49 vs train_data\n",
      "5 vs train_data\n",
      "50 vs train_data\n",
      "51 vs train_data\n",
      "52 vs train_data\n",
      "53 vs train_data\n",
      "54 vs train_data\n",
      "55 vs train_data\n",
      "56 vs train_data\n",
      "57 vs train_data\n",
      "58 vs train_data\n",
      "59 vs train_data\n",
      "6 vs train_data\n",
      "60 vs train_data\n",
      "61 vs train_data\n",
      "62 vs train_data\n",
      "63 vs train_data\n",
      "64 vs train_data\n",
      "65 vs train_data\n",
      "66 vs train_data\n",
      "67 vs train_data\n",
      "68 vs train_data\n",
      "69 vs train_data\n",
      "7 vs train_data\n",
      "70 vs train_data\n",
      "71 vs train_data\n",
      "72 vs train_data\n",
      "73 vs train_data\n",
      "74 vs train_data\n",
      "75 vs train_data\n",
      "76 vs train_data\n",
      "77 vs train_data\n",
      "78 vs train_data\n",
      "79 vs train_data\n",
      "8 vs train_data\n",
      "80 vs train_data\n",
      "81 vs train_data\n",
      "82 vs train_data\n",
      "83 vs train_data\n",
      "84 vs train_data\n",
      "85 vs train_data\n",
      "86 vs train_data\n",
      "87 vs train_data\n",
      "88 vs train_data\n",
      "89 vs train_data\n",
      "9 vs train_data\n",
      "90 vs train_data\n",
      "91 vs train_data\n",
      "92 vs train_data\n",
      "93 vs train_data\n",
      "94 vs train_data\n",
      "95 vs train_data\n",
      "96 vs train_data\n",
      "97 vs train_data\n",
      "98 vs train_data\n",
      "99 vs train_data\n",
      "Processed 12940 training files.\n",
      "Processed 5600 testing files.\n"
     ]
    }
   ],
   "source": [
    "split_dataset_into_test_and_train_sets(\"train_data\",\"data/train\",\"data/validation\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
